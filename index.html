<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hardy Chen</title>
  
  <meta name="google-site-verification" content="lxQ58o9IgoHyhIPhGGRRwtddDCbfJ0ILZSk-7pjcid4" />
  <meta name="author" content="Hardy Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hardy Chen</name>
              </p>
              <p>
                I am a first-year PhD student at <a href="https://www.ucsc.edu/">UC Santa Cruz</a>, advised by <a href="https://cihangxie.github.io/">Prof. Cihang Xie</a> and <a href="https://yuyinzhou.github.io/">Prof. Yuyin Zhou</a>. 
                My research interest lies broadly in different subareas of Vision-Language and Large Language Models.
              </p>
              <p>
                I spent 5 wonderful years at The Chinese University of Hong Kong, Shenzhen. 
                From 2023 to 2024, I worked as a full-time research engineer advised by <a href="https://wabyking.github.io/old">Prof. Benyou Wang</a>.
                From 2019 to 2023, I studied as an undergrad in School of Data Science, during which I worked shortly on AI security with <a href="https://sites.google.com/site/baoyuanwu2015/home">Prof. Baoyuan Wu</a>.
                I had unforgettable experience to work with excellent dudes in <a href="data/stories/">DY223</a>, <a href="data/stories/">DY224</a>, <a href="data/stories/">RB315</a>. 
                <!-- I was fortunate to meet the three professors: <a href="https://sites.google.com/site/baoyuanwu2015/">Baoyuan Wu</a> [<a href="data/stories/">stories</a>], <a href="https://sse.cuhk.edu.cn/en/faculty/tangxiaoying">Xiaoying Tang</a> [<a href="data/stories/">stories</a>] and <a href="https://wabyking.github.io/old">Benyou Wang</a> [<a href="data/stories/">stories</a>], who offered me great help in various aspects and meant a lot to me! -->
              </p>
              <p style="text-align:center">
                <a href="mailto:hchen403@ucsc.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.ca/citations?user=zir09KwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp -->
                 <!-- &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?hl=en&user=Gype-NsAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/andyzou_jiaming">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/g-h-chen">GitHub</a> &nbsp/&nbsp
                <!-- <a href="https://www.youtube.com/channel/UCmvUCGX9p6-azd2AetDRI8A">YouTube</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/photograph.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>What's New</heading>
              <p>[Oct 13, 2025] Check out on our geolocation benchmark <a href="https://arxiv.org/abs/2510.10880" target="_blank">EarthWhere</a>!</p>
              <p>[Oct 6, 2025] <a href="https://arxiv.org/abs/2504.11468" target="_blank">VLAA-Thinker</a> is accepted by TMLR2025!</p>
              <p>[Aug 20, 2025] <a href="https://arxiv.org/abs/2503.20271" target="_blank">ViLBench</a> is accepted by EMNLP2025 main conference! Great to collaborate with Haoqin and Weitao!</p>
              <!-- <p>[June 4, 2025] I will attend the <a href="https://workshop.humancompatible.ai/" target="_blank">CHAI2025 workshop</a>!</p> -->
              <p>[Oct 11, 2024] I will attend EMNLP2024! See y'all in Miami!</p>
              <p>[Sept 19, 2024] Two works are accepted to EMNLP2024 main conference, including <a href="https://arxiv.org/abs/2406.19280" target="_blank">HuatuoGPT-Vision</a> and my first first-authored paper on <a href="https://arxiv.org/abs/2402.10669" target="_blank">LLM Evaluation Bias</a>! It's nice to work with Shunian, <a href="https://scholar.google.com/citations?user=I0raPTYAAAAJ&hl=zh-CN" target="_blank">Junying</a>, <a href="https://wabyking.github.io/old.html" target="_blank">Benyou</a> and other co-authors!</p>
              <p>[Aug 28, 2024] Our work <a href="https://arxiv.org/abs/2402.10669" target="_blank">Humans or LLMs as the Judge? A Study on Judgement Biases</a> inspires <a href="https://lmarena.ai/" target="_blank">Chatbot Arena</a> to disentangle sytle and substance in ELO scores. Check out their <a href="https://lmsys.org/blog/2024-08-28-style-control/" target="_blank">blog</a>! </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Works</heading>
              <!-- <p>
                I'm interested in building machine learning systems that are robust, reliable, trustworthy, and understandable.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <!-- ViLBench -->
          <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/vilbench.png' width="160"></div>
                <img src='images/vilbench.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2503.20271">
                <papertitle>ViLBench: A Suite for Vision-Language Process Reward Modeling</papertitle>
              </a>
              <br>
              Haoqin Tu, Weitao Feng, <strong>Hardy Chen</strong>, Hui Liu, Xianfeng Tang, Cihang Xie
              <br>
              <em>EMNLP2025</em>
              <br>
              <em>Links:</em>
              [<a href="https://arxiv.org/abs/2503.20271">arXiv</a>]
              [<a href="https://ucsc-vlaa.github.io/ViLBench/">Website</a>]
              [<a href="https://huggingface.co/datasets/UCSC-VLAA/ViLBench/">Benchmark Dataset</a>]
              [<a href="https://huggingface.co/datasets/UCSC-VLAA/ViLReward-73K/">Reward Dataset</a>]
              <!-- [<a href="https://github.com/UCSC-VLAA/VLAA-Thinking">Code</a>] -->
              <!-- [<a href="https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard">Open LMM Reasoning Leaderboard</a>] -->
              <p></p>
              <p>
                Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal domain. To address this gap, this paper first benchmarks current vision large language models (VLLMs) as two types of reward models: output reward models (ORMs) and process reward models (PRMs) on multiple vision-language benchmarks, which reveal that neither ORM nor PRM consistently outperforms across all tasks, and superior VLLMs do not necessarily yield better rewarding performance. To further advance evaluation, we introduce ViLBench, a vision-language benchmark designed to require intensive process reward signals. Notably, OpenAI's GPT-4o with Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the benchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a promising pathway towards bridging the gap between general VLLMs and reward models --- by collecting 73.6K vision-language process reward data using an enhanced tree-search algorithm, our 3B model is able to achieve an average improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained counterpart on ViLBench by selecting OpenAI o1's generations.
              </p>
            </td>
          </tr>

          <!-- vlaa-thinker -->
          <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/vlaa_thinker.png' width="160"></div>
                <img src='images/vlaa_thinker.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2504.11468">
                <papertitle>SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models</papertitle>
              </a>
              <br>
              <strong>Hardy Chen*</strong>, Haoqin Tu*, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, Cihang Xie
              <br>
              <em>TMLR2025</em>
              <br>
              <em>Links:</em>
              [<a href="https://arxiv.org/abs/2504.11468">arXiv</a>]
              [<a href="https://ucsc-vlaa.github.io/VLAA-Thinking/">Website</a>]
              [<a href="https://huggingface.co/UCSC-VLAA/VLAA-Thinker-Qwen2.5VL-7B">Model</a>]
              [<a href="https://huggingface.co/datasets/UCSC-VLAA/VLAA-Thinking/">Dataset</a>]
              [<a href="https://github.com/UCSC-VLAA/VLAA-Thinking">Code</a>]
              [<a href="https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard">Open LMM Reasoning Leaderboard</a>]
              <p></p>
              <p>
                This work revisits the dominant supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm for training Large Vision-Language Models (LVLMs), and reveals a key finding: SFT can significantly undermine subsequent RL by inducing "pseudo reasoning paths" imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hesitant, less informative steps, and incorrect reasoning. To systematically study this effect, we introduce VLAA-Thinking, a new multimodal dataset designed to support reasoning in LVLMs. Constructed via a six-step pipeline involving captioning, reasoning distillation, answer rewrite and verification, VLAA-Thinking comprises high-quality, step-by-step visual reasoning traces for SFT, along with a more challenging RL split from the same data source. Using this dataset, we conduct extensive experiments comparing SFT, RL and their combinations. Results show that while SFT helps models learn reasoning formats, it often locks aligned models into imitative, rigid reasoning modes that impede further learning. In contrast, building on the Group Relative Policy Optimization (GRPO) with a novel mixed reward module integrating both perception and cognition signals, our RL approach fosters more genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard among 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope our findings provide valuable insights in developing reasoning-capable LVLMs and can inform future research in this area.
              </p>
            </td>
          </tr>

          <!-- milebench -->
          <!-- <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/milebench.png' width="160"></div>
                <img src='images/milebench.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2404.18532">
                <papertitle>MileBench: Benchmarking MLLMs in Long Context</papertitle>
              </a>
              <br>
              Dingjie Song, Shunian Chen, <strong>Guiming Hardy Chen</strong>, Fei Yu, Xiang Wan, Benyou Wang
              <br>
              <em>COLM2024</em>
              <br>
              <em>Links:</em>
              [<a href="https://arxiv.org/abs/2404.18532">arXiv</a>]
              [<a href="https://milebench.github.io/">Website</a>]
              [<a href="https://huggingface.co/datasets/FreedomIntelligence/MileBench/">Dataset</a>]
              [<a href="https://github.com/MileBench/MileBench">Code</a>]
              [<a href="https://milebench.github.io/#leaderboard">Leaderboard</a>]
              <p></p>
              <p>
                Despite the advancements and impressive performance of Multimodal Large Language Models (MLLMs) on benchmarks, their effectiveness in real-world, long-context, and multi-image tasks is unclear due to the benchmarks' limited scope. Existing benchmarks often focus on single-image and short-text samples, and when assessing multi-image tasks, they either limit the image count or focus on specific task (e.g time-series captioning), potentially obscuring the performance challenges of MLLMs. To address these limitations, we introduce MileBench, a pioneering benchmark designed to test the MultImodal Long-contExt capabilities of MLLMs. This benchmark comprises not only multimodal long contexts, but also multiple tasks requiring both comprehension and generation. We establish two distinct evaluation sets, diagnostic and realistic, to systematically assess MLLMs' long-context adaptation capacity and their ability to complete tasks in long-context scenarios. Our experimental results, obtained from testing 22 models, revealed that while the closed-source GPT-4o outperforms others, most open-source MLLMs struggle in long-context situations. Interestingly, the performance gap tends to widen with an increase in the number of images. We strongly encourage an intensification of research efforts towards enhancing MLLMs' long-context capabilities, especially in scenarios involving multiple images.
              </p>
            </td>
          </tr> -->

          <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/allava.png' width="160"></div>
                <img src='images/allava.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2402.11684">
                <papertitle>ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models</papertitle>
              </a>
              <br>
              <strong>Guiming Hardy Chen</strong>, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, Benyou Wang
              <br>
              <em>arXiv</em>
              <br>
              <em>Links:</em>
              [<a href="https://arxiv.org/abs/2402.11684">arXiv</a>]
              [<a href="https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V">Dataset</a>]
              [<a href="https://huggingface.co/FreedomIntelligence/ALLaVA-Phi3-mini-128k">Model</a>]
              [<a href="https://allava.freedomai.cn/#/">Demo</a>]
              [<a href="https://github.com/FreedomIntelligence/ALLaVA">Code</a>]
              <p></p>
              <p>
                Large vision-language models (LVLMs) have shown premise in a broad range of vision-language tasks with their strong reasoning and generalization capabilities. However, they require considerable computational resources for training and deployment. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To this end, we propose a comprehensive pipeline for generating a synthetic dataset. The key idea is to leverage strong proprietary models to generate (i) fine-grained image annotations for vision-language alignment and (ii) complex reasoning visual question-answering pairs for visual instruction fine-tuning, yielding 1.3M samples in total. We train a series of lite VLMs on the synthetic dataset and experimental results demonstrate the effectiveness of the proposed scheme, where they achieve competitive performance on 17 benchmarks among 4B LVLMs, and even perform on par with 7B/13B-scale models on various benchmarks. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. We name our dataset <strong>ALLaVA</strong>, and open-source it to research community for developing better resource-efficient LVLMs for wider usage.
              </p>
            </td>
          </tr>


          <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/humans_or_llm_judges.png' width="160"></div>
                <img src='images/humans_or_llm_judges.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2402.10669">
                <papertitle>Humans or LLMs as the Judge? A Study on Judgement Biases</papertitle>
              </a>
              <br>
              <strong>Guiming Hardy Chen</strong>*, Shunian Chen*, Ziche Liu, Feng Jiang, Benyou Wang
              <br>
              <em>EMNLP2024</em>
              <br>
              <em>Links:</em>
              [<a href="https://arxiv.org/abs/2402.10669">arXiv</a>]
              <!-- [<a href="https://github.com/FreedomIntelligence/ALLaVA">Code</a>] -->
              <p></p>
              <p>
                Adopting human and large language models (LLM) as judges (a.k.a human- and LLM-as-a-judge) for evaluating the performance of LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLMs, questioning the reliability of the evaluation results. In this paper, we propose a novel framework that is free from referencing groundtruth annotations for investigating Misinformation Oversight Bias, Gender Bias, Authority Bias and Beauty Bias on LLM and human judges. We curate a dataset referring to the revised Bloom's Taxonomy and conduct thousands of evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases. We further exploit these biases to conduct attacks on LLM judges. We hope that our work can notify the community of the bias and vulnerability of human- and LLM-as-a-judge, as well as the urgency of developing robust evaluation systems.
              </p>
            </td>
          </tr>
          

          <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/cmb.png' width="160"></div>
                <img src='images/cmb.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2308.08833">
                <papertitle>CMB: A Comprehensive Medical Benchmark in Chinese</papertitle>
              </a>
              <br>
              Xidong Wang*, <strong>Guiming Hardy Chen</strong>*, Dingjie Song*, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, Haizhou Li
              <br>
              <em>NAACL2024</em>
              <br>
              <em>Links:</em>
              [<a href="https://arxiv.org/abs/2308.08833">arXiv</a>]
              [<a href="https://github.com/FreedomIntelligence/CMB">Code</a>]
              <p></p>
              <p>
                Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in contextual incongruities to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at this https URL.
              </p>
            </td>
          </tr>
          
          
          <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/bert_clip_synthesia.png' width="160"></div>
                <img src='images/bert_clip_synthesia.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2306.03678">
                <papertitle>On the Difference of BERT-style and CLIP-style Text Encoders</papertitle>
              </a>
              <br>
              Zhihong Chen*, <strong>Guiming Hardy Chen</strong>*, Shizhe Diao, Xiang Wan, Benyou Wang
              <br>
              <em>ACL2023 Findings</em>
              <br>
              <em>Links:</em>
              [<a href="https://arxiv.org/abs/2306.03678">arXiv</a>]
              [<a href="https://github.com/zhjohnchan/bert-clip-synesthesia">Code</a>]
              <p></p>
              <p>
                Masked language modeling (MLM) has been one of the most popular pretraining recipes in natural language processing, e.g., BERT, one of the representative models. Recently, contrastive language-image pretraining (CLIP) has also attracted attention, especially its vision models that achieve excellent performance on a broad range of vision tasks. However, few studies are dedicated to studying the text encoders learned by CLIP. In this paper, we analyze the difference between BERT-style and CLIP-style text encoders from three experiments: (i) general text understanding, (ii) vision-centric text understanding, and (iii) text-to-image generation. Experimental analyses show that although CLIP-style text encoders underperform BERT-style ones for general text understanding tasks, they are equipped with a unique ability, i.e., synesthesia, for the cross-modal association, which is more similar to the senses of humans.
              </p>
            </td>
          </tr>
          

          <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/phoenix.png' width="160"></div>
                <img src='images/phoenix.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.10453">
                <papertitle>Phoenix: Democratizing ChatGPT across Languages</papertitle>
              </a>
              <br>
              Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, <strong>Guiming Hardy Chen</strong>, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, Haizhou Li              <br>
              <em>arXiv</em>
              <br>
              <em>Links:</em>
              [<a href="https://arxiv.org/abs/2304.10453">arXiv</a>]
              [<a href="https://github.com/FreedomIntelligence/LLMZoo">Code</a>]
              <p></p>
              <p>
                This paper presents our efforts to democratize ChatGPT across language. We release a large language model "Phoenix", achieving competitive performance among open-source English and Chinese models while excelling in languages with limited resources (covering both Latin and non-Latin languages). We believe this work will be beneficial to make ChatGPT more accessible, especially in countries where people cannot use ChatGPT due to restrictions from OpenAI or local goverments.
              </p>
            </td>
          </tr>


          <!-- <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/repe.png' width="160"></div>
                <img src='images/repe.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ai-transparency.org/">
                <papertitle>Representation Engineering: A Top-Down Approach to AI Transparency</papertitle>
              </a>
              <br>
              <strong>Andy Zou</strong>, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, Zico Kolter, Dan Hendrycks
              <br>
              <em>arXiv</em>
              <br>
              <em>Selected Talks:</em>
              [<a href="https://openai.com/">OpenAI</a>]
              [<a href="https://deepmind.google/">Google</a>]
              [<a href="https://ai.meta.com/">Meta</a>]
              [<a href="https://labs.oracle.com/pls/apex/r/labs/labs/intro">Oracle</a>]
              [<a href="https://www.caltech.edu/">Caltech</a>]
              [<a href="https://mitibmwatsonailab.mit.edu/">MIT-IBM</a>]
              [<a href="https://aished.org/">AISHED</a>]
              [<a href="https://pioneeringminds.ai/">PMAG</a>]
              <br>
              <em>Links:</em>
              [<a href="https://arxiv.org/abs/2310.01405">arXiv</a>]
              [<a href="https://www.ai-transparency.org/">Demo</a>]
              [<a href="https://github.com/andyzoujm/representation-engineering">Code</a>]
              [<a href="https://www.foxnews.com/us/researchers-shed-light-how-read-control-ai-systems-minds?fbclid=IwAR2tz842osxnwju_Kk8i0YCsWWc8Rn6Q2cX3pfbK5T0RBQiJ3zhgf2cCLi8_aem_AUp3P_0m0jMChtrkIrghFTM2PvTCVrWPz9fy3suzkWbgrnjMMirot4TwxBZZ0XDUnbg">Fox</a>]
              <p></p>
              <p>
                In this paper, we introduce and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including truthfulness, memorization, power-seeking, and more, demonstrating the promise of representation-centered transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
              </p>
            </td>
          </tr> -->


          <!-- <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/gcg.png' width="160"></div>
                <img src='images/gcg.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://llm-attacks.org/">
                <papertitle>Universal and Transferable Adversarial Attacks on Aligned Language Models</papertitle>
              </a>
              <br>
              <strong>Andy Zou</strong>, Zifan Wang, J. Zico Kolter, Matt Fredrikson
              <br>
              <em>arXiv</em>
              <br>
              <em>Selected Talks:</em>
              [<a href="https://www.utoronto.ca/">UofT</a>]
              [<a href="https://cohere.com/research">Cohere</a>]
              [<a href="https://buzzrobot.com/">BuzzRobot</a>]
              [<a href="https://www.cognitiverevolution.ai/">Cognitive Revolution</a>]
              <br>
              <em>Links:</em>
              [<a href="https://arxiv.org/abs/2307.15043">arXiv</a>]
              [<a href="https://llm-attacks.org/">Demo</a>]
              [<a href="https://github.com/llm-attacks/llm-attacks">Code</a>]
              [<a href="https://www.nytimes.com/2023/07/27/technology/researchers-poke-holes-in-safety-controls-of-chatgpt-and-other-chatbots.html">NY Times</a>]
              [<a href="https://www.theregister.com/2023/07/27/llm_automated_attacks/">The Register</a>]
              [<a href="https://www.wired.com/story/ai-adversarial-attacks/">Wired</a>]
              [<a href="https://www.express.co.uk/news/world/1813236/ai-engineers-san-francisco-ai-deadliest-virus">Daily Express</a>]
              [<a href="https://www.cnn.com/videos/business/2023/08/15/hackers-defcon-ai-chat-gpt-google-bard-donie-pkg-biz-vpx.cnn">CNN</a>]
              
              <p></p>
              <p>
                We found adversarial suffixes that completely circumvent the alignment of open source LLMs. More concerningly, the same prompts transfer to ChatGPT, Claude, Bard, and LLaMA-2...
              </p>
            </td>
          </tr> -->

          <!-- <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/machi.png' width="160"></div>
                <img src='images/machi.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://arxiv.org/abs/2304.03279">
                <papertitle>Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark</papertitle>
              </a>
              <br>
              Alexander Pan*, Chan Jun Shern*, <strong>Andy Zou*</strong>, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, Dan Hendrycks
              <br>
              <em>ICML 2023 Oral</em>
              <br>
              [<a href="https://arxiv.org/abs/2304.03279">arXiv</a>]
              [<a href="https://github.com/aypan17/machiavelli">Code</a>]
              [<a href="https://aypan17.github.io/machiavelli/">Website</a>]
              <p></p>
              <p>
                Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities.
              </p>
            </td>
          </tr> -->
          
          <!-- <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/forecasting.png' width="160"></div>
                <img src='images/forecasting.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }
  
                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://arxiv.org/abs/2206.15474">
                <papertitle>Forecasting Future World Events with Neural Networks</papertitle>
              </a>
              <br>
              <strong>Andy Zou</strong>, Tristan Xiao, Ryan Jia, Joe Kwon, Mantas Mazeika, Richard Li, Dawn Song, Jacob Steinhardt, Owain Evans, Dan Hendrycks
              <br>
              <em>NeurIPS 2022</em>
              <br>
              [<a href="http://arxiv.org/abs/2206.15474">arXiv</a>]
              [<a href="https://github.com/andyzoujm/autocast">Code</a>]
              <p></p>
              <p>
                Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.
              </p>
            </td>
          </tr> -->

          <!-- <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/wellbeing.jpg' width="160"></div>
                <img src='images/wellbeing.jpg' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }

                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=jbdp9m7nr0R">
                <papertitle>How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios</papertitle>
              </a>
              <br>
              Mantas Mazeika*, Eric Tang*, <strong>Andy Zou</strong>, Steven Basart, Jun Shern Chan, Dawn Song, David Forsyth, Jacob Steinhardt, Dan Hendrycks
              <br>
							<em>NeurIPS 2022 Oral</em>
              <br>
              [<a href="https://openreview.net/pdf?id=jbdp9m7nr0R">arXiv</a>]
              [<a href="https://github.com/hendrycks/emodiversity">Code</a>]
              <p></p>
              <p>
                As video understanding becomes widely used in real-world applications, a key consideration is developing human-centric systems that understand not only the content of the video but also how it would affect the wellbeing and emotional state of viewers. To facilitate research in this setting, we introduce two large-scale datasets with over 60,000 videos manually annotated for subjective wellbeing and emotional response. In experiments, we show how video models that are largely trained to recognize actions and find contours of objects can be repurposed to understand human preferences and the emotional content of videos. We hope our datasets can help foster further advances at the intersection of commonsense video understanding and human preference learning.
              </p>
            </td>
          </tr> -->

          <!-- <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/fractal.png' width="160"></div>
                <img src='images/fractal.png' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }

                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.05135">
                <papertitle>PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures</papertitle>
              </a>
              <br>
              Dan Hendrycks*, <strong>Andy Zou*</strong>, Mantas Mazeika, Leonard Tang, Dawn Song, and Jacob Steinhardt
              <br>
							<em>CVPR 2022</em>
              <br>
              [<a href="https://arxiv.org/abs/2112.05135">arXiv</a>]
              [<a href="https://github.com/andyzoujm/pixmix/">Code</a>]
              <p></p>
              <p>
                In real-world applications of machine learning, reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out-of-distribution (OOD) robustness, prediction consistency, resilience to adversaries, calibrated uncertainty estimates, and the ability to detect anomalous inputs. To meet this challenge, we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals, which outperforms numerous baselines, is near Pareto-optimal, and roundly improves safety measures.
              </p>
            </td>
          </tr> -->

          <!-- <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/ood.jpg' width="160"></div>
                <img src='images/ood.jpg' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }

                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1911.11132">
                <papertitle>Scaling Out-of-Distribution Detection for Real-World Settings</papertitle>
              </a>
              <br>
              Dan Hendrycks, Steven Basart, Mantas Mazeika, <strong>Andy Zou</strong>, Joe Kwon, Mohammadreza Mostajabi, Dawn Song, Jacob Steinhardt
              <br>
							<em>ICML 2022</em>
              <br>
              [<a href="https://arxiv.org/abs/1911.11132">arXiv</a>]
              [<a href="https://github.com/hendrycks/anomaly-seg">Code</a>]
              <p></p>
              <p>
                To set the stage for more realistic out-of-distribution detection, we depart from small-scale settings and explore large-scale multiclass and multi-label settings with high-resolution images and thousands of classes. To make future work in real-world settings possible, we create new benchmarks for three large-scale settings.
              </p>
            </td>
          </tr> -->

          <!-- <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/jiminy.jpg' width="160"></div>
                <img src='images/jiminy.jpg' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }

                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2110.13136">
                <papertitle>What Would Jiminy Cricket Do? Towards Agents That Behave Morally</papertitle>
              </a>
              <br>
              Dan Hendrycks*, Mantas Mazeika*, <strong>Andy Zou</strong>, Sahil Patel, Christine Zhu, Jesus Navarro, Dawn Song, Bo Li, Jacob Steinhardt
              <br>
							<em>NeurIPS 2021</em>
              <br>
              [<a href="https://arxiv.org/abs/2110.13136">arXiv</a>]
              [<a href="https://github.com/hendrycks/jiminy-cricket">Code</a>]
              <p></p>
              <p>
                To facilitate the development of agents that avoid causing wanton harm, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of diverse, morally salient scenarios. By annotating every possible game state, the Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. Using models with commonsense moral knowledge, we create an elementary artificial conscience that assesses and guides agents. In extensive experiments, we find that the artificial conscience approach can steer agents towards moral behavior without sacrificing performance.
              </p>
            </td>
          </tr> -->

          <!-- <tr onmouseout="survey_stop()" onmouseover="survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='survey_image'>
                  <img src='images/multitask.jpg' width="160"></div>
                <img src='images/multitask.jpg' width="160">
              </div>
              <script type="text/javascript">
                function survey_start() {
                  document.getElementById('survey_image').style.opacity = "1";
                }

                function survey_stop() {
                  document.getElementById('survey_image').style.opacity = "0";
                }
                survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2009.03300">
                <papertitle>Measuring Massive Multitask Language Understanding</papertitle>
              </a>
              <br>
              Dan Hendrycks, Collin Burns, Steven Basart, <strong>Andy Zou</strong>, Mantas Mazeika, Dawn Song, Jacob Steinhardt
              <br>
							<em>ICLR 2021</em>
              <br>
              [<a href="https://arxiv.org/abs/2009.03300">arXiv</a>]
              [<a href="https://github.com/hendrycks/test">Code</a>]
              <p></p>
              <p>
                We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.
              </p>
            </td>
          </tr> -->

          

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Service</heading>
            <p>
              <li>2025: ACL2025, NAACL2025, COLM2025, ACL2025 Student Research Workshop</li>
              <li>2024: EMNLP2024</li>
            </p>
          </td>
        </tr>
      </tbody></table>

      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Awards</heading>
          <p>
            <li>EMNLP2024 Outstanding Reviewer</li>
          </p>
        </td>
      </tr>
    </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Other Interests</heading>
            <p>
              I practiced Taekwondo from 2008 to 2015 and received 1st Dan Black Belt. I was a badminton college team member during my undergrad. 
              I practiced clarinet for years when I was young but currently I'm more into guitar for its chorus. I am more than interested in piano and music theory but haven't got a chance to learn them formally.
            </p>
            <p>
              I am an introverted person yet open towards different culture and values. 
              I speak multiple languages/dialects, including English, Mandarin, Teochew, Cantonese and a bit of Malay and Indonesian. Hope to explore more in the future : )
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/website">Website template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
        
      </td>
    </tr>
  </table>
  
  <table style="width:25%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
        <br>
        <p>
          <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=G1L0wcMmrUGmhd0DfxKpJ_G_G-V1T7Aa2U_Z7zHQ5gg&cl=ffffff&w=a"></script>
        </p>
      </td>
    </tr>
  </tbody></table>
  <p>
  </p>
  

  
</body>
</html>
